---
title: "412_Housing"
author: "David Sun, Harrison DiStefano, Yuxiu Zheng"
date: "11/26/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(dplyr)
library(glmnet)
library(car)
library(MASS)

# rmse function 
rmse <- function(y_hat, y){round(sqrt(mean((y_hat - y)^2)),3)}

# rmse rsquare rss function 
rmse_r2_rss <- function(y, predict, df) {
  RSS <- sum((predict - y)^2)
  TSS <- sum((y - mean(y))^2)
  #R_square <- 1 - RSS / SST
  RMSE = sqrt(RSS/nrow(df))
  n=nrow(df)
  p=ncol(df)-1
  R_square = 1 - (RSS/(n - p))/(TSS/(n - 1)) #adjusted R 2
  # Model performance metrics
data.frame(
  RMSE = round(RMSE,5),
  Rsquare = round(R_square,5)
)
}

aic_bic <- function(fit){
tLL <- fit$nulldev - deviance(fit)
k <- fit$df
n <- fit$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)

BIC<-log(n)*k - tLL

data.frame(AICc,BIC,fit$lambda)
}

```

#Examine Basic Data Frame
```{r}
df<- read.csv('USA_Housing.csv')
names(df) <- c("AvgIncome", "AvgHouseAge", "AvgNumRooms", "AvgNumBedrooms", "AreaPopulation", "Price", "Address")

dim(df)         # 5000 X 7
df$state <- sub(".*\\b([A-Z]{2}) \\d{5}.*", "\\1", df[,7])  # get the state from address
head(df,5)
df1<-df[,c(1:6,8)] # remove address column
head(df1,5)

df1 %>%
dplyr::count(df1$state, sort = TRUE)
```
In our original data, we have 5000 records with 7 variables. We extracted state from Address variable 

```{r}
s_df1<-summary(df1)
s_df1
```


#Data Cleaning
## Na's
```{r}
sum(is.na(df1))  # count how many NA's
```
no NA's

##Outlier

###Price
```{r}
boxplot(df1$Price)
iqr_price<-IQR(df1$Price)
df2 = subset(df1, Price>=quantile(df1$Price, 0.25)-1.5*iqr_price & Price<=quantile(df1$Price, 0.75)+1.5*iqr_price )
summary(df2)
boxplot(df2$Price)
```

Since Price is our response variable, we start with it first by using 1.5 * IQR method to remove outliers

We removed 35 outliers.

### Average Income 
```{r}
boxplot(df2$AvgIncome)
iqr_ainc<-IQR(df2$AvgIncome)
df3 = subset(df2, AvgIncome>=quantile(df2$AvgIncome, 0.25)-1.5*iqr_ainc & AvgIncome<=quantile(df2$AvgIncome, 0.75)+1.5*iqr_ainc )
summary(df3)
boxplot(df3$AvgIncome)
```
We removed 29 more outliers on Average Income.

For the rest of the variables, even they have outliers bases on IQR method, but they do not affect our models much. 



#EDA 
###Correlation
```{r}
df_corr<-cor(df1)
df_corr
corrplot(df_corr, method = "circle")
```

We see AvgNumRooms and AvgNumBedrooms are highly corrected, and by intuition, more rooms would have more bedrooms. Hypothetically, we will remove AvgNumRooms in our models, later will be shown in our analysis. We also see AvgIncome is closely correlated with Price.  


###Price

```{r}
qplot(df1$Price,xlab = 'Price',main = 'Price Hist',col ='red',bins = 30) # price
```

Price 1m -1.5m

###Avg Area Income
```{r}
qplot(df3$AvgIncome,xlab = 'Avg Area Income',bins = 20,col ='red')
```
60k-80k

###Average House Age
```{r}
qplot(df3$AvgHouseAge,xlab = 'Avg House Age',main ='Avg House Age Hist',bins = 9,col ='red')
```
5-7 years old

###Average Number of Rooms

```{r}
qplot(df1$AvgNumRooms,xlab = 'Avg Num Rooms', main ='Avg Num Rooms Hist',bins = 10,col ='red')
```

###Average Number of Bedrooms
```{r}
qplot(df1$AvgNumBedrooms,xlab = 'Avg Num Bedrooms',main = 'Avg Num Bedrooms Hist',bins = 7,col ='red')
```
2-5

Area Population
```{r}
qplot(df1$AreaPopulation,xlab = 'Area Population',bins = 20,col ='red')
```
30k-45k


#Models
##Train/Test Seperation
```{r}
set.seed(1)

dt = sort(sample(nrow(df3), nrow(df3)*.7))

#train
df_tr <- df3[dt,-7] #remove state
x_tr <- data.matrix(df_tr[,c(-6,-7)])
y_tr <-df_tr[,6]

#test
df_ts <-  df3[-dt,-7]
x_ts <-data.matrix(df_ts[,c(-6,-7)])
y_ts <-df_ts[,6]

```


##Linear Regression (Harrison)

```{r collinearity}
# eigenvalue method
numeric_feature <- df3[, c(1:5)]

X <- as.matrix(numeric_feature)
XtX <- t(X) %*% X

eigen(XtX)
```



```{r lm}
# Linear Model
lm_housing <- lm(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AvgNumBedrooms + AreaPopulation, data=df_tr)
plot(lm_housing)
summary(lm_housing)

# VIF
car::vif(lm_housing)

#train and test
predictions<- predict(lm_housing, newx = x_tr)
rmse_r2_rss(y_tr,predictions,x_tr)

predictions1<- predict(lm_housing, newx = x_ts)
rmse_r2_rss(y_ts,predictions1,x_ts)
```

Since all features except average number of bedrooms are significant and it is correlated with the average number of rooms, let's remove that feature from the model.

```{r lm2}
lm_housing2 <- lm(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation, data=df_tr)
plot(lm_housing2)
summary(lm_housing2)

# VIF
car::vif(lm_housing2)

#train and test
df_tr1<-df_tr[,-4]
df_ts1<-df_ts[,-4]



rmse_r2_rss(df_tr1$Price,predict(lm_housing2,df_tr1),df_tr1)
rmse_r2_rss(df_ts1$Price,predict(lm_housing2,df_ts1),df_ts1)



```

##Ridge (Yuxiu)
```{r}
set.seed(1)
numeric_features <- as.matrix(df_tr[, c(1:5)])

lambdas_to_try <- seq(0, 2, by = 0.001)
ridge_housing_cv <- cv.glmnet(x_tr, y_tr, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)
min_lambda_ridcv <- ridge_housing_cv$lambda.min
ridge_housing <- glmnet(x_tr, y_tr, alpha = 0, lambda = ridge_housing_cv$lambda.min, standardize = TRUE)

adj_R2 <- function(data, resids, n, p){
  TSS = sum((data - mean(data))^2)
  RSS = sum(resids^2)
  adj_R2 = 1 - (RSS/(n - p))/(TSS/(n - 1))
  return(cat("The adjusted R^2 is:", adj_R2, "\n"))
}

adj_R2(df_tr$Price, df_tr$Price - predict(ridge_housing, newx = numeric_features), nrow(df_tr), ncol(df_tr)-1)

predictions<- predict(ridge_housing, s = min_lambda_ridcv, newx = x_tr)
rmse_r2_rss(y_tr,predictions,x_tr)

predictions<- predict(ridge_housing, s = min_lambda_ridcv, newx = x_ts)
rmse_r2_rss(y_ts,predictions,x_ts)
```

The adjusted R^2 is about $0.913$, which is close to 1.


##Lasso (David)
```{r, warning = FALSE}
set.seed(2)
#CV
lambdas_to_try <- seq(0, 1, by = 0.001)
lsmod_cv <- cv.glmnet(x_tr, y_tr, alpha = 1, lambda = lambdas_to_try, 
                      standardize = TRUE, nfolds = 10)
min_lambda_lscv <- lsmod_cv $lambda.min
min_lambda_lscv

lsmod_cv $lambda.1se

predictions<- predict(lsmod_cv, s = min_lambda_lscv, newx = x_tr)
rmse_r2_rss(y_tr,predictions,x_tr)

predictions<- predict(lsmod_cv, s = min_lambda_lscv, newx = x_ts)
rmse_r2_rss(y_ts,predictions,x_ts)

```

