---
title: "412_Housing"
author: "David Sun, Harrison DiStefano, Yuxiu Zheng"
date: "11/26/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(dplyr)
library(glmnet)
library(car)
library(MASS)
library(caret) 
library(ggcorrplot)
library(qpcR)
# rmse function 
rmse <- function(y_hat, y){round(sqrt(mean((y_hat - y)^2)),3)}

# rmse rsquare rss function 
rmse_r2_rss <- function(y, predict, df) {
  RSS <- sum((predict - y)^2)
  TSS <- sum((y - mean(y))^2)
  #R_square <- 1 - RSS / SST
  RMSE = sqrt(RSS/nrow(df))
  n=nrow(df)
  p=ncol(df)-1
  R_square = 1 - (RSS/(n - p))/(TSS/(n - 1)) #adjusted R 2
  # Model performance metrics
data.frame(
  RMSE = round(RMSE,5),
  Rsquare = round(R_square,5)
)
}

aic_bic <- function(fit){
tLL <- fit$nulldev - deviance(fit)
k <- fit$df
n <- fit$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)

BIC<-log(n)*k - tLL

data.frame(AICc,BIC,fit$lambda)
}

```

#Examine Basic Data Frame
```{r}
df<- read.csv('USA_Housing.csv')
names(df) <- c("AvgIncome", "AvgHouseAge", "AvgNumRooms", "AvgNumBedrooms", "AreaPopulation", "Price", "Address")

dim(df)         # 5000 X 7
df$state <- sub(".*\\b([A-Z]{2}) \\d{5}.*", "\\1", df[,7])  # get the state from address
head(df,5)
df1<-df[,c(1:6,8)] # remove address column
head(df1,5)

df1 %>%
dplyr::count(df1$state, sort = TRUE)
```
In our original data, we have 5000 records with 7 variables. We extracted state from Address variable 

```{r}
s_df1<-summary(df1)
s_df1
```


#Data Cleaning
## Na's
```{r}
sum(is.na(df1))  # count how many NA's
```
no NA's

##Outlier

###Price
```{r}
boxplot(df1$Price)
iqr_price<-IQR(df1$Price)
df2 = subset(df1, Price>=quantile(df1$Price, 0.25)-1.5*iqr_price & Price<=quantile(df1$Price, 0.75)+1.5*iqr_price )
summary(df2)
boxplot(df2$Price)

```

Since Price is our response variable, we start with it first by using 1.5 * IQR method to remove outliers

We removed 35 outliers.



### Average Income 
```{r}
boxplot(df2$AvgIncome)
iqr_ainc<-IQR(df2$AvgIncome)
df3 <- subset(df2, AvgIncome>=quantile(df2$AvgIncome, 0.25)-1.5*iqr_ainc & AvgIncome<=quantile(df2$AvgIncome, 0.75)+1.5*iqr_ainc )
summary(df3)
boxplot(df3$AvgIncome)
```
We removed 29 more outliers on Average Income.

For the rest of the variables, even they have outliers bases on IQR method, but they do not affect our models much. 

subset
```{r}
#df4<-df3
df4 <- subset(df3, state == "CA" | state == "TX"|state == "NY"| state == "FL"| state == "IL" )
summary(df4)


df5<-df4 %>% mutate(gdp_ratio =
                     case_when(state == "CA" ~ 3.3, 
                               state == "TX" ~ 2.0,
                               state == "NY" ~ 1.9,
                               state == "FL" ~ 1.2,
                               state == "IL" ~ 0.94)
)
```

#EDA 
###Correlation
```{r}
df_corr<-cor(df5[, -c(7)])
df_corr
#corrplot(df_corr, method = "circle")
ggcorrplot(df_corr)
```

We see AvgNumRooms and AvgNumBedrooms are highly corrected, and by intuition, more rooms would have more bedrooms. Hypothetically, we will remove AvgNumRooms in our models, later will be shown in our analysis. We also see AvgIncome is closely correlated with Price.  


###Price

```{r}
qplot(df5$Price,xlab = 'Price',main = 'Price Hist',col ='red',bins = 30) # price
quantile(df5$Price, 0.75)
```

Price 1m -1.5m

###Avg Area Income
```{r}
qplot(x=df5$AvgIncome,xlab = 'Avg Area Income',main="Avg Area Income Hist",bins = 20,col ='red')
plot(x=df5$AvgIncome,y=df5$Price,xlab = 'Avg Area Income',ylab='Price',main = 'Price v Avg Area Income',col ='red')
abline(lm(Price~AvgIncome,data = df4),col= "Blue")
quantile(df5$AvgIncome, 0.25)
```
60k-80k

###Average House Age
```{r}
qplot(df4$AvgHouseAge,xlab = 'Avg House Age',main ='Avg House Age Hist',bins = 9,col ='red')

plot(x=df4$AvgHouseAge,y=df4$Price,xlab = 'Avg House Age',ylab='Price',main = 'Price v Avg House Age',col ='purple')
abline(lm(Price~AvgHouseAge,data = df4),col= "Blue")
quantile(df4$AvgHouseAge, 0.25)
quantile(df4$AvgHouseAge, 0.75)
```
5-7 years old

###Average Number of Rooms

```{r}
qplot(df4$AvgNumRooms,xlab = 'Avg Num Rooms', main ='Avg Num Rooms Hist',bins = 10,col ='red')

plot(x=df4$AvgNumRooms,y=df4$Price,xlab = 'Avg Num Rooms',ylab='Price',main = 'Price v Avg Num Rooms',col ='Black')
abline(lm(Price~AvgNumRooms,data = df4),col= "Blue")
quantile(df4$AvgNumRooms, 0.25)
quantile(df4$AvgNumRooms, 0.75)
```

###Average Number of Bedrooms
```{r}
qplot(df4$AvgNumBedrooms,xlab = 'Avg Num Bedrooms',main = 'Avg Num Bedrooms Hist',bins = 7,col ='red')
plot(x=df4$AvgNumBedrooms,y=df4$Price,xlab = 'Avg Num Bedrooms',ylab='Price',main = 'Price v Avg Num Bedrooms',col ='green')
abline(lm(Price~AvgNumBedrooms,data = df4),col= "Blue")
quantile(df4$AvgNumBedrooms, 0.25)
quantile(df4$AvgNumBedrooms, 0.75)
```
2-5

Area Population
```{r}
qplot(df4$AreaPopulation,xlab = 'Area Population',bins = 20,col ='red')

plot(x=df4$AreaPopulation,y=df4$Price,xlab = 'Area Population',ylab='Price',main = 'Price v Area Population',col ='red')
abline(lm(Price~AreaPopulation,data = df4),col= "Blue")
```
30k-45k


#Models
##Train/Test Seperation
```{r}
set.seed(1)

dt = sort(sample(nrow(df5), nrow(df5)*.8))

#train
df_tr <- df5[dt,-7] #remove state
x_tr <- data.matrix(df_tr[,c(-6)])
y_tr <-df_tr[,6]

#test
df_ts <-  df5[-dt,-7]
x_ts <-data.matrix(df_ts[,c(-6)])
y_ts <-df_ts[,6]

```


##Linear Regression (Harrison)

```{r collinearity}
# eigenvalue method
numeric_feature <- df4[, c(1:5)]

X <- as.matrix(numeric_feature)
XtX <- t(X) %*% X

eigen_decomp = eigen(XtX)
eigen_decomp$values
```


```{r lm}
# Linear Model
lm_housing <- lm(Price ~ ., data=df_tr)
plot(lm_housing)
summary(lm_housing)

# VIF
car::vif(lm_housing)

#train and test
rmse_r2_rss(df_tr$Price,predict(lm_housing,df_tr),df_tr)
rmse_r2_rss(df_ts$Price,predict(lm_housing,df_ts),df_ts)
```

From AIC step function, avgnumbedrooms and GDP_ratio variables are removed. For avgnumbedrooms, it is reasonable because by intuition, all features except average number of bedrooms are significant and it is correlated with the average number of rooms.

```{r lm2}
lm_housing2<-step(lm_housing,trace = FALSE)
lm_housing2$coefficients

#lm_housing2 <- lm(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation, data=df_tr)
plot(lm_housing2)
summary(lm_housing2)

# VIF
car::vif(lm_housing2)

#train and test
df_tr1<-df_tr[,-c(4,7)]
df_ts1<-df_ts[,-c(4,7)]


rmse_r2_rss(df_tr1$Price,predict(lm_housing2,df_tr1),df_tr1)
rmse_r2_rss(df_ts1$Price,predict(lm_housing2,df_ts1),df_ts1)


# CV 
set.seed(1)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
model_cv <- train(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation, data=df_tr1, method="lm",
                  trControl = train.control)
print(model_cv)
rmse_r2_rss(df_tr1$Price,predict(model_cv,df_tr1),df_tr1)
rmse_r2_rss(df_ts1$Price,predict(model_cv,df_ts1),df_ts1)
# Summarize the results


```


##Ridge (Yuxiu)
```{r}
set.seed(1)
numeric_features <- as.matrix(df_tr[, -c(6)])

lambdas_to_try <- seq(0, 2, by = 0.001)
ridge_housing_cv <- cv.glmnet(x_tr, y_tr, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)
min_lambda_ridcv <- ridge_housing_cv$lambda.min
ridge_housing <- glmnet(x_tr, y_tr, alpha = 0, lambda = ridge_housing_cv$lambda.min, standardize = TRUE)

adj_R2 <- function(data, resids, n, p){
  TSS = sum((data - mean(data))^2)
  RSS = sum(resids^2)
  adj_R2 = 1 - (RSS/(n - p))/(TSS/(n - 1))
  return(cat("The adjusted R^2 is:", adj_R2, "\n"))
}

#adj_R2(df_tr$Price, df_tr$Price - predict(ridge_housing, newx = numeric_features), nrow(df_tr), ncol(df_tr)-1)

predictions<- predict(ridge_housing, s = min_lambda_ridcv, newx = x_tr)
rmse_r2_rss(y_tr,predictions,x_tr)

predictions<- predict(ridge_housing, s = min_lambda_ridcv, newx = x_ts)
rmse_r2_rss(y_ts,predictions,x_ts)
```

The adjusted R^2 is about $0.9147$, which is close to 1.


##Lasso (David)
```{r, warning = FALSE}
set.seed(2)
#CV
lambdas_to_try <- seq(0, 1, by = 0.001)
lsmod_cv <- cv.glmnet(x_tr, y_tr, alpha = 1, lambda = lambdas_to_try, 
                      standardize = TRUE, nfolds = 10)
min_lambda_lscv <- lsmod_cv $lambda.min
min_lambda_lscv

lsmod_cv $lambda.1se

plot(lsmod_cv$glmnet.fit,xvar = "lambda", label = T, main ="lsmod_cv$glmnet.fit")
plot(lsmod_cv,xvar = "lambda", label = T, main = "lsmod_cv")


predictions<- predict(lsmod_cv, s = min_lambda_lscv, newx = x_tr)
rmse_r2_rss(y_tr,predictions,x_tr)

predictions<- predict(lsmod_cv, s = min_lambda_lscv, newx = x_ts)
rmse_r2_rss(y_ts,predictions,x_ts)

```



##PCR (David)
```{r,warning =FALSE}
library(pls)
set.seed(1)

pcr_m <-prcomp(x_tr) # remove response siri

plot(pcr_m$sdev[1:10],type="l", ylab="SD of PC", xlab="PC number")

pcrmod_cv <- pcr(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation, data=df_tr,validation = "CV", scale = TRUE, ncomp =4,segments = 10)
summary(pcrmod_cv)


pcrmse_2_cv <- RMSEP(pcrmod_cv, estimate="CV")
min1<-which.min(pcrmse_2_cv$val)-1 # -1 for intercept
min1
 
predictions<- predict(pcrmod_cv, df_tr, ncomp = min1)
rmse_r2_rss(y_tr, predictions, x_tr)

predictions<- predict(pcrmod_cv, df_ts, ncomp = min1)
rmse_r2_rss(y_ts, predictions, x_ts)



###################################

pcrmod_cv2 <- pcr(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation, data=df_tr,validation = "CV", scale = TRUE, ncomp =3,segments = 10)
summary(pcrmod_cv2)

pcrmse_2_cv2 <- RMSEP(pcrmod_cv2, estimate="CV")
min1<-which.min(pcrmse_2_cv2$val)-1 # -1 for intercept
min1
 
predictions<- predict(pcrmod_cv2, df_tr, ncomp = min1)
rmse_r2_rss(y_tr, predictions, x_tr)

predictions<- predict(pcrmod_cv2, df_ts, ncomp = min1)
rmse_r2_rss(y_ts, predictions, x_ts)

#######################################################
pcrmod_cv3 <- pcr(Price ~ AvgIncome + AvgHouseAge + AvgNumRooms + AreaPopulation+gdp_ratio, data=df_tr,validation = "CV", scale = TRUE, ncomp =5,segments = 10)
summary(pcrmod_cv)


pcrmse_2_cv3 <- RMSEP(pcrmod_cv3, estimate="CV")
min1<-which.min(pcrmse_2_cv3$val)-1 # -1 for intercept
min1

predictions<- predict(pcrmod_cv3, df_tr, ncomp = min1)
rmse_r2_rss(y_tr, predictions, x_tr)

predictions<- predict(pcrmod_cv3, df_ts, ncomp = min1)
rmse_r2_rss(y_ts, predictions, x_ts)

round(PRESS(pcrmod_cv)$P.square,3)#4  0.737
round(PRESS(pcrmod_cv2)$P.square,3)#3 0.649
round(PRESS(pcrmod_cv3 )$P.square,3)#5 0.799
```


## Polynomial Regression (David )
```{r}
poly_model1 <- lm(df_tr$Price~polym(AvgIncome , AvgHouseAge , AvgNumRooms , AreaPopulation,gdp_ratio,degree=4,raw=TRUE),data=df_tr)

rmse_r2_rss(df_tr$Price,predict(poly_model1,df_tr),df_tr)
rmse_r2_rss(df_ts$Price,predict(poly_model1,df_ts),df_ts)

#############################################################
poly_model2 <- lm(df_tr$Price~polym(AvgIncome , AvgHouseAge , AvgNumRooms , AreaPopulation,gdp_ratio,degree=3,raw=TRUE),data=df_tr)

rmse_r2_rss(df_tr$Price,predict(poly_model2,df_tr),df_tr)
rmse_r2_rss(df_ts$Price,predict(poly_model2,df_ts),df_ts)
#############################################################
poly_model3 <- lm(df_tr$Price~polym(AvgIncome , AvgHouseAge , AvgNumRooms , AreaPopulation,gdp_ratio,degree=2,raw=TRUE),data=df_tr)

rmse_r2_rss(df_tr$Price,predict(poly_model3,df_tr),df_tr)
rmse_r2_rss(df_ts$Price,predict(poly_model3,df_ts),df_ts)



```


## RF
```{r}
set.seed(1)
library(randomForest)
rf <- randomForest(
  Price ~ .,
  data=df_tr
)

pred = predict(rf, newdata=df_tr[,-6])
rmse_r2_rss(df_tr$Price,pred,df_tr)

pred = predict(rf, newdata=df_ts[,-6])
rmse_r2_rss(df_ts$Price,pred,df_ts)
```

